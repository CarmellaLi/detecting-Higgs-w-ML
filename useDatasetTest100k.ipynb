{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import matplotlib \n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_json(\"datasetTest100k.json\", typ=\"frame\", lines = True, precise_float=True,nrows = 5*100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from RealisticDataExplorer.ipynb\n",
    "trkData = np.stack(data[3],axis=0)\n",
    "\n",
    "trk_high_level_name = [\"track_2_d0_significance\", \"track_3_d0_significance\",\n",
    "                       \"track_2_z0_significance\", \"track_3_z0_significance\",\n",
    "                       \"n_tracks_over_d0_threshold\", \"jet_prob\", \"jet_width_eta\", \"jet_width_phi\"]\n",
    "\n",
    "for i, trkName in enumerate(trk_high_level_name):\n",
    "    tmpArr = np.array(trkData[:,i],dtype=\"float32\")\n",
    "    data[trkName] = tmpArr\n",
    "\n",
    "vtxData = np.stack(data[4],axis=0)\n",
    "    \n",
    "vtx_high_level_name = [\"vertex_significance\", \"n_secondary_vertices\", \"n_secondary_vertex_tracks\",\n",
    "                       \"delta_r_vertex\", \"vertex_mass\", \"vertex_energy_fraction\"]\n",
    "\n",
    "for i, vtxName in enumerate(vtx_high_level_name):\n",
    "    tmpArr = np.array(vtxData[:,i],dtype=\"float32\")\n",
    "    data[vtxName] = tmpArr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "jets_trk = []\n",
    "for i in range(0,200000):\n",
    "    trkVars = []\n",
    "    for j in range(len(data[5][i])):\n",
    "        trkVars.append(data[5][i][j][0])\n",
    "    jets_trk.append(trkVars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "d0 = []\n",
    "for i in jets_trk:\n",
    "    temp = []\n",
    "    for j in i:\n",
    "        temp.append(j[0])\n",
    "    d0.append(temp)\n",
    "\n",
    "for i in d0:\n",
    "    while len(i)<15:\n",
    "        i.append(0)\n",
    "    while len(i)>15:\n",
    "        i.remove(min(i))\n",
    "X_d0 = torch.tensor(d0,dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sortTrk (data):\n",
    "    for i in data:\n",
    "        i.sort(key = lambda ls : np.abs(ls[0]))\n",
    "\n",
    "def padding (data, n, length):\n",
    "    temp = []\n",
    "    for i in range(length):\n",
    "        temp.append(0)\n",
    "    for i in data:\n",
    "        while len(i)<n:\n",
    "            i.append(temp)\n",
    "        while len(i)>n:\n",
    "            i.remove(i[len(i)-1])\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sortTrk(jets_trk)\n",
    "padding(jets_trk, 15, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = data[2]\n",
    "y = y.values.tolist()\n",
    "\n",
    "for i in range(len(y)):\n",
    "    if y[i]==5:\n",
    "        y[i]=1\n",
    "    else:\n",
    "        y[i]=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_trk = torch.tensor(jets_trk, dtype=torch.float32)\n",
    "X_trk = torch.flatten(X_trk,start_dim=1)\n",
    "y = torch.tensor(y, dtype=torch.float32).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusting learning rate of group 0 to 1.0000e-02.\n",
      "Adjusting learning rate of group 0 to 1.0000e-02.\n",
      "Finished epoch 0, latest loss 0.6850210428237915\n",
      "Adjusting learning rate of group 0 to 1.0000e-02.\n",
      "Finished epoch 1, latest loss 0.6833359003067017\n",
      "Adjusting learning rate of group 0 to 1.0000e-02.\n",
      "Finished epoch 2, latest loss 0.7024783492088318\n",
      "Adjusting learning rate of group 0 to 1.0000e-02.\n",
      "Finished epoch 3, latest loss 0.6848697662353516\n",
      "Adjusting learning rate of group 0 to 1.0000e-02.\n",
      "Finished epoch 4, latest loss 0.7082681059837341\n",
      "Adjusting learning rate of group 0 to 1.0000e-02.\n",
      "Finished epoch 5, latest loss 0.686998724937439\n",
      "Adjusting learning rate of group 0 to 1.0000e-02.\n",
      "Finished epoch 6, latest loss 0.6759951114654541\n",
      "Adjusting learning rate of group 0 to 1.0000e-02.\n",
      "Finished epoch 7, latest loss 0.6866924166679382\n",
      "Adjusting learning rate of group 0 to 1.0000e-02.\n",
      "Finished epoch 8, latest loss 0.7135294079780579\n",
      "Adjusting learning rate of group 0 to 1.0000e-02.\n",
      "Finished epoch 9, latest loss 0.6899705529212952\n",
      "Adjusting learning rate of group 0 to 1.0000e-02.\n",
      "Finished epoch 10, latest loss 0.6916865706443787\n",
      "Adjusting learning rate of group 0 to 1.0000e-02.\n",
      "Finished epoch 11, latest loss 0.6791374087333679\n",
      "Adjusting learning rate of group 0 to 1.0000e-02.\n",
      "Finished epoch 12, latest loss 0.6765725612640381\n",
      "Adjusting learning rate of group 0 to 1.0000e-02.\n",
      "Finished epoch 13, latest loss 0.6867175102233887\n",
      "Adjusting learning rate of group 0 to 1.0000e-02.\n",
      "Finished epoch 14, latest loss 0.6952526569366455\n",
      "Adjusting learning rate of group 0 to 1.0000e-02.\n",
      "Finished epoch 15, latest loss 0.6833492517471313\n",
      "Adjusting learning rate of group 0 to 1.0000e-02.\n",
      "Finished epoch 16, latest loss 0.6996694207191467\n",
      "Adjusting learning rate of group 0 to 1.0000e-02.\n",
      "Finished epoch 17, latest loss 0.6835998296737671\n",
      "Adjusting learning rate of group 0 to 1.0000e-02.\n",
      "Finished epoch 18, latest loss 0.6828164458274841\n",
      "Adjusting learning rate of group 0 to 1.0000e-02.\n",
      "Finished epoch 19, latest loss 0.6867190599441528\n",
      "Adjusting learning rate of group 0 to 1.0000e-02.\n",
      "Finished epoch 20, latest loss 0.6949697136878967\n",
      "Adjusting learning rate of group 0 to 1.0000e-02.\n",
      "Finished epoch 21, latest loss 0.6845983266830444\n",
      "Adjusting learning rate of group 0 to 1.0000e-02.\n",
      "Finished epoch 22, latest loss 0.6847074627876282\n",
      "Adjusting learning rate of group 0 to 1.0000e-02.\n",
      "Finished epoch 23, latest loss 0.6847209334373474\n",
      "Adjusting learning rate of group 0 to 1.0000e-02.\n",
      "Finished epoch 24, latest loss 0.6932812333106995\n",
      "Adjusting learning rate of group 0 to 1.0000e-02.\n",
      "Finished epoch 25, latest loss 0.6845676898956299\n",
      "Adjusting learning rate of group 0 to 1.0000e-02.\n",
      "Finished epoch 26, latest loss 0.6967206597328186\n",
      "Adjusting learning rate of group 0 to 1.0000e-02.\n",
      "Finished epoch 27, latest loss 0.6865605115890503\n",
      "Adjusting learning rate of group 0 to 1.0000e-02.\n",
      "Finished epoch 28, latest loss 0.6899599432945251\n",
      "Adjusting learning rate of group 0 to 1.0000e-02.\n",
      "Finished epoch 29, latest loss 0.6967639327049255\n",
      "Adjusting learning rate of group 0 to 1.0000e-02.\n",
      "Finished epoch 30, latest loss 0.7018609642982483\n",
      "Adjusting learning rate of group 0 to 1.0000e-02.\n",
      "Finished epoch 31, latest loss 0.6985348463058472\n",
      "Adjusting learning rate of group 0 to 1.0000e-02.\n",
      "Finished epoch 32, latest loss 0.6980651617050171\n",
      "Adjusting learning rate of group 0 to 1.0000e-02.\n",
      "Finished epoch 33, latest loss 0.6985722184181213\n",
      "Adjusting learning rate of group 0 to 1.0000e-02.\n",
      "Finished epoch 34, latest loss 0.6899572014808655\n",
      "Adjusting learning rate of group 0 to 1.0000e-02.\n",
      "Finished epoch 35, latest loss 0.6932805776596069\n",
      "Adjusting learning rate of group 0 to 1.0000e-02.\n",
      "Finished epoch 36, latest loss 0.6962576508522034\n",
      "Adjusting learning rate of group 0 to 1.0000e-02.\n",
      "Finished epoch 37, latest loss 0.689950168132782\n",
      "Adjusting learning rate of group 0 to 1.0000e-02.\n",
      "Finished epoch 38, latest loss 0.6815406084060669\n",
      "Adjusting learning rate of group 0 to 1.0000e-02.\n",
      "Finished epoch 39, latest loss 0.683278501033783\n",
      "Adjusting learning rate of group 0 to 1.0000e-02.\n",
      "Finished epoch 40, latest loss 0.698627769947052\n",
      "Adjusting learning rate of group 0 to 1.0000e-02.\n",
      "Finished epoch 41, latest loss 0.6802307367324829\n",
      "Adjusting learning rate of group 0 to 1.0000e-02.\n",
      "Finished epoch 42, latest loss 0.684776246547699\n",
      "Adjusting learning rate of group 0 to 1.0000e-02.\n",
      "Finished epoch 43, latest loss 0.6810215711593628\n",
      "Adjusting learning rate of group 0 to 1.0000e-02.\n",
      "Finished epoch 44, latest loss 0.709343433380127\n",
      "Adjusting learning rate of group 0 to 1.0000e-02.\n",
      "Finished epoch 45, latest loss 0.6934024691581726\n",
      "Adjusting learning rate of group 0 to 1.0000e-02.\n",
      "Finished epoch 46, latest loss 0.6932933330535889\n",
      "Adjusting learning rate of group 0 to 1.0000e-02.\n",
      "Finished epoch 47, latest loss 0.688255250453949\n",
      "Adjusting learning rate of group 0 to 1.0000e-02.\n",
      "Finished epoch 48, latest loss 0.7035642862319946\n",
      "Adjusting learning rate of group 0 to 1.0000e-02.\n",
      "Finished epoch 49, latest loss 0.6899616122245789\n",
      "Adjusting learning rate of group 0 to 1.0000e-02.\n",
      "Finished epoch 50, latest loss 0.6899510025978088\n",
      "Adjusting learning rate of group 0 to 1.0000e-02.\n",
      "Finished epoch 51, latest loss 0.6982465982437134\n",
      "Adjusting learning rate of group 0 to 1.0000e-02.\n",
      "Finished epoch 52, latest loss 0.6807716488838196\n",
      "Adjusting learning rate of group 0 to 1.0000e-02.\n",
      "Finished epoch 53, latest loss 0.7072682976722717\n",
      "Adjusting learning rate of group 0 to 1.0000e-02.\n",
      "Finished epoch 54, latest loss 0.6882835626602173\n",
      "Adjusting learning rate of group 0 to 1.0000e-02.\n",
      "Finished epoch 55, latest loss 0.6798338294029236\n",
      "Adjusting learning rate of group 0 to 1.0000e-02.\n",
      "Finished epoch 56, latest loss 0.6830904483795166\n",
      "Adjusting learning rate of group 0 to 1.0000e-02.\n",
      "Finished epoch 57, latest loss 0.6866048574447632\n",
      "Adjusting learning rate of group 0 to 1.0000e-02.\n",
      "Finished epoch 58, latest loss 0.686774492263794\n",
      "Adjusting learning rate of group 0 to 1.0000e-02.\n",
      "Finished epoch 59, latest loss 0.6952576637268066\n",
      "Adjusting learning rate of group 0 to 1.0000e-02.\n",
      "Finished epoch 60, latest loss 0.6736317276954651\n",
      "Adjusting learning rate of group 0 to 1.0000e-02.\n",
      "Finished epoch 61, latest loss 0.691602885723114\n",
      "Adjusting learning rate of group 0 to 1.0000e-02.\n",
      "Finished epoch 62, latest loss 0.6814637184143066\n",
      "Adjusting learning rate of group 0 to 1.0000e-02.\n",
      "Finished epoch 63, latest loss 0.6883109211921692\n",
      "Adjusting learning rate of group 0 to 1.0000e-02.\n",
      "Finished epoch 64, latest loss 0.6782371997833252\n",
      "Adjusting learning rate of group 0 to 1.0000e-02.\n",
      "Finished epoch 65, latest loss 0.699931800365448\n",
      "Adjusting learning rate of group 0 to 1.0000e-02.\n",
      "Finished epoch 66, latest loss 0.6697096228599548\n",
      "Adjusting learning rate of group 0 to 1.0000e-02.\n",
      "Finished epoch 67, latest loss 0.6989397406578064\n",
      "Adjusting learning rate of group 0 to 1.0000e-02.\n",
      "Finished epoch 68, latest loss 0.6969171762466431\n",
      "Adjusting learning rate of group 0 to 1.0000e-02.\n",
      "Finished epoch 69, latest loss 0.679370641708374\n",
      "Adjusting learning rate of group 0 to 1.0000e-02.\n",
      "Finished epoch 70, latest loss 0.6882849335670471\n",
      "Adjusting learning rate of group 0 to 1.0000e-02.\n",
      "Finished epoch 71, latest loss 0.6809490323066711\n",
      "Adjusting learning rate of group 0 to 1.0000e-02.\n",
      "Finished epoch 72, latest loss 0.6760911345481873\n",
      "Adjusting learning rate of group 0 to 1.0000e-02.\n",
      "Finished epoch 73, latest loss 0.6865488290786743\n",
      "Adjusting learning rate of group 0 to 1.0000e-02.\n",
      "Finished epoch 74, latest loss 0.6899909377098083\n",
      "Adjusting learning rate of group 0 to 1.0000e-02.\n",
      "Finished epoch 75, latest loss 0.6796352863311768\n",
      "Adjusting learning rate of group 0 to 1.0000e-02.\n",
      "Finished epoch 76, latest loss 0.6950796246528625\n",
      "Adjusting learning rate of group 0 to 1.0000e-02.\n",
      "Finished epoch 77, latest loss 0.6828525066375732\n",
      "Adjusting learning rate of group 0 to 1.0000e-02.\n",
      "Finished epoch 78, latest loss 0.6864503622055054\n",
      "Adjusting learning rate of group 0 to 1.0000e-02.\n",
      "Finished epoch 79, latest loss 0.7032260894775391\n",
      "Adjusting learning rate of group 0 to 9.5500e-03.\n",
      "Finished epoch 80, latest loss 0.6949793100357056\n",
      "Adjusting learning rate of group 0 to 9.1000e-03.\n",
      "Finished epoch 81, latest loss 0.6916255950927734\n",
      "Adjusting learning rate of group 0 to 8.6500e-03.\n",
      "Finished epoch 82, latest loss 0.6865637898445129\n",
      "Adjusting learning rate of group 0 to 8.2000e-03.\n",
      "Finished epoch 83, latest loss 0.6851477026939392\n",
      "Adjusting learning rate of group 0 to 7.7500e-03.\n",
      "Finished epoch 84, latest loss 0.6916398406028748\n",
      "Adjusting learning rate of group 0 to 7.3000e-03.\n",
      "Finished epoch 85, latest loss 0.6983225345611572\n",
      "Adjusting learning rate of group 0 to 6.8500e-03.\n",
      "Finished epoch 86, latest loss 0.6761631965637207\n",
      "Adjusting learning rate of group 0 to 6.4000e-03.\n",
      "Finished epoch 87, latest loss 0.68830406665802\n",
      "Adjusting learning rate of group 0 to 5.9500e-03.\n",
      "Finished epoch 88, latest loss 0.6816638112068176\n",
      "Adjusting learning rate of group 0 to 5.5000e-03.\n",
      "Finished epoch 89, latest loss 0.6846361756324768\n",
      "Adjusting learning rate of group 0 to 5.0500e-03.\n",
      "Finished epoch 90, latest loss 0.6916838884353638\n",
      "Adjusting learning rate of group 0 to 4.6000e-03.\n",
      "Finished epoch 91, latest loss 0.6744809150695801\n",
      "Adjusting learning rate of group 0 to 4.1500e-03.\n",
      "Finished epoch 92, latest loss 0.6967085003852844\n",
      "Adjusting learning rate of group 0 to 3.7000e-03.\n",
      "Finished epoch 93, latest loss 0.6949409246444702\n",
      "Adjusting learning rate of group 0 to 3.2500e-03.\n",
      "Finished epoch 94, latest loss 0.6812981963157654\n",
      "Adjusting learning rate of group 0 to 2.8000e-03.\n",
      "Finished epoch 95, latest loss 0.689963698387146\n",
      "Adjusting learning rate of group 0 to 2.3500e-03.\n",
      "Finished epoch 96, latest loss 0.6831509470939636\n",
      "Adjusting learning rate of group 0 to 1.9000e-03.\n",
      "Finished epoch 97, latest loss 0.6916278004646301\n",
      "Adjusting learning rate of group 0 to 1.4500e-03.\n",
      "Finished epoch 98, latest loss 0.6848387122154236\n",
      "Adjusting learning rate of group 0 to 1.0000e-03.\n",
      "Finished epoch 99, latest loss 0.6967670321464539\n",
      "tensor([[0.4576],\n",
      "        [0.4576],\n",
      "        [0.4575],\n",
      "        ...,\n",
      "        [0.4575],\n",
      "        [0.4575],\n",
      "        [0.4576]])\n",
      "Accuracy 0.5422599911689758\n"
     ]
    }
   ],
   "source": [
    "Ndimentions=15\n",
    "n_epochs=100\n",
    "batch_size=100\n",
    "\n",
    "train_ds = TensorDataset(X_d0,y)\n",
    "train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "NNmodel = nn.Sequential(\n",
    "    nn.Linear(Ndimentions,Ndimentions),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(p=0.3),\n",
    "    nn.Linear(Ndimentions,Ndimentions),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(p=0.3),\n",
    "    nn.Linear(Ndimentions,Ndimentions),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(Ndimentions,Ndimentions),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(Ndimentions,Ndimentions),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(Ndimentions,Ndimentions),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(Ndimentions,Ndimentions),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(Ndimentions,Ndimentions),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(Ndimentions,Ndimentions),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(Ndimentions,1),\n",
    "    nn.Sigmoid()\n",
    "    )\n",
    "\n",
    "def lr_lambda(epoch):\n",
    "    if epoch <= 80:\n",
    "        return 1\n",
    "    else:\n",
    "        return 100*(-0.00045*epoch+0.046)\n",
    "        \n",
    "loss_fn = nn.BCELoss()\n",
    "optimizer = optim.SGD(NNmodel.parameters(), lr = 0.01)\n",
    "lr_scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lr_lambda, verbose=True)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    NNmodel.train()\n",
    "    for xb, yb in train_dl:\n",
    "        y_pred = NNmodel(xb)\n",
    "        loss = loss_fn(y_pred, yb)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    lr_scheduler.step()\n",
    "    print(f'Finished epoch {epoch}, latest loss {loss}')\n",
    "\n",
    "with torch.no_grad():\n",
    "    y_pred = NNmodel(X_d0)\n",
    "print(y_pred)\n",
    "accuracy = (y_pred.round() == y).float().mean()\n",
    "print(f\"Accuracy {accuracy}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
